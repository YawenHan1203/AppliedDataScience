---
title: "Applied Data Science:  Midterm Project"
author: "Yawen Han(yh3069), Anqi Wang(aw3136), Xuran Jia(xj2222)"
date: "03/14/2019"
output:
  prettydoc::html_pretty:
  theme: cayman
highlight: github
---

```{r setup, include=FALSE}
set.seed(73)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, comment="", warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=55))
```

```{r libraries, echo = FALSE}
library(data.table)
library(DT)
```

```{r source_files}
training.data.file <- "../Data/MNIST-fashion training set-49.csv"
testing.data.file <- "../Data/MNIST-fashion testing set-49.csv"

```


```{r load_data}

train.dat <- fread(input = training.data.file, verbose = FALSE)
test.dat <- fread(input = testing.data.file, verbose = FALSE)
```

```{r constants}
n.values <- c(500, 1000, 2000)
iterations <- 3
train.dat.size <- train.dat[,.N]
```

```{r round_numerics_function}
# Function round numerical vars to required decimal
round.numerics <- function(x, digits){
  if(is.numeric(x)){
    x <- round(x = x, digits = digits)
  }
  return(x)
}
```


```{r generate_samples}
# Fuction getting some random values to use here
find.sample.index <- function(train.dat, sample.size) {
  index <- 1:train.dat[,.N]
  sample1.index <- sample(x = index,size = sample.size,replace = FALSE)
  # Remove the sampled items from the vector of values
  sample2.index <- sample(x = index[-c(sample1.index)],size = sample.size,replace = FALSE)
  # Another sample, and another removal
  sample3.index <- sample(x = index[-c(sample1.index,sample2.index)],size = sample.size,replace = FALSE)
  return(list(sample1.index,sample2.index,sample3.index))
}

```



```{r iteration_sample_for_each_model}
# Function that iterate all sample sizes for the given model
# return a list and a datatable
iteration.samples.each.model <- function(model){
  #initialize a list to store model results
  ls = list()
  #iterarte all samples
  for (i in 1:iterations){
    for (j in 1:iterations){
      TrainSet <- sprintf("dat_%d_%d", n.values[i], j)
      # call model function to compute score
      score <- get(model)(get(TrainSet),test.dat)
      ls[3*(i-1)+j] <- list(c(model,n.values[i],TrainSet,score))
    }
  }
  #transform list of list to data.table
  dat.result= setDT(as.data.frame(t(as.data.frame(ls))))
  #set column names
  setnames(dat.result,c("Model","Sample Size","Data","A","B","C","Points"))
  #convert factor to numeric for columns "A","B","C","Points"
  dat.result[, c("A","B","C","Points"):=lapply(X=.SD, as.character), .SDcols=c("A","B","C","Points")]
  dat.result[, c("A","B","C","Points"):=lapply(X=.SD, as.numeric), .SDcols=c("A","B","C","Points")]
  return (dat.result)
}


```


```{r scoring_function}
# Function used to calculate the score of fitted model
scoring.funnction <- function(sample.size, running.time, accuracy){
  A <- sample.size/train.dat.size
  B <- min(running.time/60.0, 1)
  C <- 1-accuracy
  score <- 0.25*A + 0.25*B +0.5*C
  return (c(A,B,C,score))
}

```



## Introduction

This project focus on an image recognition on the MNIST Fashion database (https://github.com/zalandoresearch/fashion-mnist), which collected a large number of images for different types of apparel. Each image is divided into small squares called pixels of equal area. Within each pixel, a brightness measurement was recorded in grayscale. The brightness values range from 0 (white) to 255 (black). The original data set divided each image into 784 (28 by 28) pixels.

In this project, the team created and evaluated different machine learning models on different sample sizes. The quality of different combinations of the models and the sample sizes can be compared based on their Points. The overall goal is to build a classiffication method that minimizes the value of Points. In this setting, the ideal algorithm would use as little data as possible, implement the computation as quickly as possible, and accurately classify as many items in the testing set as possible. Therefore, there are likely to be trade-offs between the sample size, running time, and accuracy.

The ten machine learning models applied here are: 1)Random Forest, 2) Ridge Regression, 3) Generalized Boosted Regression, 4) Support Vector Machines, 5) K-Nearest Neighbors, 6) Multinomial Logistic Regression, 7) Classification Tree, 8) Lasso Regression, 9) Neural Networks, 10) Ensembling Model. The 3 different sample sizes selected are 500, 1000, 2000; and the overall points are computed based on the scoring function: points = 0.25$*$A+0.25$*$B+0.5$*$C. For each model, the detailed evaluation and explanations are given along with the results table. Then, the overall results summary and scoreboard are summarized at the end of the report. Further discussion and evaluations are also performed to have a better understanding of the performance for the 10 models. 


## Initial Evaluation

```{r dimension}
dim.train<-dim(train.dat)
dim.test<-dim(test.dat)
sprintf("The training set has %d rows, %d columns", dim.train[1],dim.train[2] )
sprintf("The testing set has %d rows, %d columns.", dim.test[1],dim.test[2])
```

According to the output above, the dimension of training set is (`r dim.train`), and the dimension of testing set is (`r dim.test`). 

The column names for the two datatables are: `r names(train.dat)`.

Then, we want to check if there is any na value or non-numeric value exist in the dataset:


```{r clean_data}
#check NA value
train.na.check <- sum(is.na(train.dat))
test.na.check <- sum(is.na(test.dat))
#check numeric
train.numeric.check <- sum(is.numeric(as.matrix(train.dat[,2:50]))==F)
test.numeric.check <- sum(is.numeric(as.matrix(test.dat[,2:50]))==F)

#combine check results
datatable(data.table("Check1:#na in trainset"=train.na.check,"Check2: #na in testset"=test.na.check,"Check3:#nonnumeric in trainset"=train.numeric.check,"Check4: #nonnumeric in testset"=test.numeric.check), rownames=FALSE)

```

Before modeling the dataset, we first perform an initial inspection of the dataset. As the table shown above: neither na values nor non-numeric values are found in the dataset. 


```{r generate_sample_sets}
#change the datatype of label column as factor
train.dat[,label:=factor(label)]
test.dat[,label:=factor(label)]

#iteration for all possible sample sizes
for (i in 1:iterations){
  #call function  find.sample.index
  #get sample indexes for three samples with required sample size
  sample.index <- find.sample.index(train.dat, n.values[i])
  #get three samples
  for (j in 1:iterations){
    #name with pattern
    sample.name <- sprintf("dat_%d_%d", n.values[i], j)
    #assign samples with names
    assign(eval(sample.name), train.dat[unlist(sample.index[j]),])
  }
}
```

As the team will create and evaluate different machine learning models on different sample sizes in the following sections, samples are generated in this process. The 3 different sample sizes that the team work with are 500, 1000, 2000. For each sample size, 3 separate model development sets by sampling from the rows of the overall training data randomly without replacement. Therefore, total 9 different sample sets are used to evaluate train the models we developed later. Using a nameing structure like **dat_n_k** to represent the training set at sample size **n** with the **k**th sample size. 

Thus, the samples below are generated from the original dataset as data.tables. These tables are used to create models in the following sections:

```{r samples_table}
datatable(data.table("Sample Size" = c(500,1000,2000),
  "First Random Sample" = c("dat_500_1","dat_500_2","dat_500_3"),
  "Second Random Sample" = c("dat_1000_1","dat_1000_2","dat_1000_3"),
  "Third Random Sample" = c("dat_2000_1","dat_2000_2","dat_2000_3")))

```

***

### Model 1 - Random Forest

```{r code_model1_development, eval = TRUE}
library(randomForest)
#Function for RandomForest Model
random.forest <- function(TrainSet,TestSet){
  sample.size <- TrainSet[,.N] #sample size
  #RandomForest Model
  start.time <- Sys.time()# record start time
  model <- randomForest(formula = label~. , data = TrainSet)
  # Predicting on Validation set
  predTest <- predict(model, TestSet, type = "response")
  end.time <- Sys.time()# record end time
  running.time <- as.numeric(end.time - start.time)  # model running time
  # Checking classification accuracy
  accuracy <- mean(predTest == TestSet[,label])
  #calculate score of fitted model
  score <- scoring.funnction(sample.size, running.time, accuracy)
  return (score)
}

```


```{r load_model1}
# get result table
random.forest.result <- iteration.samples.each.model("random.forest")
# show as HTML table
datatable(random.forest.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])

```

Random Forest

Random Forests is an ensemble learning method by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It usually performs good by correcting the overfitting problem of decision trees. We apply the *randomForest* function from the *randomForest* package to 9 sample training sets with the default parameters’ settings. 

The table above shows the results and points for each sample. It is observed that as the sample size multiplied by two, the running time also multiplied twice. The accuracy of the model is also keep increasing with sample size. However, the total points decreased first, then increased later and achieved the optimal at sample size 1000. Thus, for RandomForest model, the 1000 sample size seems has the best performance based on our grading criteria. 

***

### Model 2 - Ridge Regression 


```{r code_model2_development, eval = TRUE}
library(glmnet)
#Function for RidgeRegression Model
ridge.regression <- function(TrainSet,TestSet){
  sample.size <- TrainSet[,.N]
  #RidgeRegression Model
  start.time <- Sys.time()# record start time
  model <- cv.glmnet(x=data.matrix(TrainSet[,2:50]), y=TrainSet[,label], alpha=0,type.measure = "class",family="multinomial")
  # Predicting on test set
  predTest <- predict(model, data.matrix(TestSet[,2:50]), type="class", s="lambda.min")
  end.time <- Sys.time()# record end time
  running.time <- as.numeric(end.time-start.time) # model running time
  # Checking classification accuracy
  accuracy <- mean(predTest == TestSet[,label]) 
  #calculate score of fitted model
  score <- scoring.funnction(sample.size, running.time, accuracy)
  return (score)
}
```


```{r load_model2}
# get result table
ridge.regression.result <- iteration.samples.each.model("ridge.regression")
# show as HTML table
datatable(ridge.regression.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

Ridge Regression

Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. The form of the penalty is $\ell_{2}$ norm of a coefficient. We apply *cv.glment* function from *glment* package and set the parameter of  alpha equal to 0. Also, we use the predict function to predict the labels. *cv.glmnet* can find the best lambda value based on 10-fold cross validation, then fit the ridge regression model. Cross validation may reduce the model variance.

The table above shows the results and points for each sample. It is observed that as the sample size multiplied by two, the running time also multiplied twice. The accuracy of the model is also keep increasing with sample size, but the improvement is not very significnt with only 1% each increment of sample size. What’s more, the total points keep increasing with the sample size, as a result of the higher contribution of running time. In other words, the sample size has a more significant influence on the running time for ridge regression model instead of accuracy. Therefore, for Ridge Regression model, the 500 sample size seems has the best performance based on our grading criteria. 


***

### Model 3 - Generalized Boosted Regression Model  


```{r code_model3_development, eval = TRUE}
library(gbm)
#Function for BoostedRegression Model
boosted.regression <- function(TrainSet,TestSet){
  sample.size <- TrainSet[,.N]
  #BoostedRegression Model
  start.time <- Sys.time()# record start time
  # fit BoostedRegression Model
  model <- gbm(label ~ ., data=TrainSet, distribution="multinomial", cv.folds=5)  
  # Predicting on test set
  predTest <- predict(model, newdata=TestSet, n.trees=100, type = "response")
  #print (predTest)
  end.time <- Sys.time()# record end time
  running.time <- as.numeric(end.time-start.time) # model running time
  predTest <- setDT(as.data.frame(predTest))
  
  predTest <- predTest[, colnames(.SD)[max.col(.SD, ties.method = "first")]]
  predTest <- sub(".100","",x=predTest)# remove pattern
  
  # Checking classification accuracy
  accuracy <- mean(predTest == TestSet[,label]) 
  #calculate score of fitted model
  score <- scoring.funnction(sample.size, running.time, accuracy)
  return (score)
}
```


```{r load_model3}
# get result table
boosted.regression.result <- iteration.samples.each.model("boosted.regression")
# show as HTML table
datatable(boosted.regression.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

Generalized Boosted Regression Model

Gradient boosted machines are an extremely popular machine learning algorithm that have proven successful across many domains. Whereas random forests build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms. We apply *gbm* function from *gbm* package to 9 sample training sets, and use the cross validation method with 5-fold to find the number of trees with the best performance. The 5-fold cross validation find the best n.trees as 100, and use that to make predictions for the test set and evaluate results.

The table above shows the results and points for each sample. It is observed that as the sample size multiplied by two, the running time for sample size 500 and 1000 do not have big difference, while the running time for sample size 2000 multiplied twice. The accuracy of the model keeps increasing with sample size. However, the total points do not have significant change as sample size increases. Thus, for Boosted Regression model, all sample sizes seem have similar performance based on our grading criteria. 


***

### Model 4 - Support Vector Machine


```{r code_model4_development, eval = TRUE}
library(e1071)
#Function for svm model
model.svm <-function(TrainSet,TestSet){
   sample.size <- TrainSet[,.N]
   start.time <-Sys.time() #record start time
   #SVM model
   mod.svm <-svm(label~.,data=TrainSet)
   #Predicting on test set
   predTest <-predict(mod.svm,newdata=TestSet[,2:50])
   end.time <-Sys.time() #record end time
   running.time <-as.numeric(end.time-start.time) #model running time
   #Checking classification accuracy
   accuracy <- mean(predTest == TestSet[,label])
   #Calculate Score
   score <-scoring.funnction(sample.size,running.time,accuracy)
   return(score)
}
```


```{r load_model4}
# get result table
svm.result <- iteration.samples.each.model("model.svm")
# show as HTML table
datatable(svm.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

Supprt Vector Machine

SVM model scales relatively well to the high dimensional data and it also performs well on classification problem.  At first, for each sample size 500, 1000 and 2000, we generated 3 separate training datasets, so there are nine training datasets in total. We built a svm model using *svm()* function from *e1071* package and predicted the labels using *predict()* function. Running time was calculated by subtracting the start time from end time. At the end, we compared our prediction with the actual value in the testing set.

We got low misclassification rate with around 24% using sample 500 training set, around 22% using sample 1000 training set and 20% using sample 2000 training set. Finally, scores were calculated by the formula based on the sample size, running time and accuracy each on different weight. Based on the scores that we got, we think the models using svm are performed well since we obtained low scores which are average around 0.12. Furthermore, we will choose the training dataset with sample size 2000 to train my final model since the average score of the three models with sample size 2000 is the lowest as compared to the model trained by 500 and 1000 samples. The best thing about using SVM is that it has generalization in practice, reducing the risk of overfitting. However, on the other hand, choosing a good kernel function might be difficult and it is also hard for us to understand and interpret the final model thereby hindering us to draw deeper insight from it. 


***

### Model 5 - K-Nearest Neighbors


```{r code_model5_development, eval = TRUE}
library(caret)
#Function for knn model
knn <-function(TrainSet,TestSet){
    sample.size <- TrainSet[,.N]
    start.time <-Sys.time() #record the start time
    ctrl <- trainControl(method="repeatedcv",repeats = 10)  
    knnFit <- train(label ~ ., data = TrainSet, method = "knn", 
                trControl = ctrl, preProcess = c("center","scale")) #tune the parameter k with 10-fold cross validation
    predTest <-predict(knnFit, newdata = TestSet[,2:50]) #predict using the best k in knnFit
    end.time <-Sys.time() #record the end time
    running.time <-as.numeric(end.time-start.time)
    #Checking classification accuracy
    accuracy <- mean(predTest == TestSet[,label])
    #Calculate Score
    score <-scoring.funnction(sample.size,running.time,accuracy)
    return(score)
}

```

```{r load_model5}
# get result table
knn.result <- iteration.samples.each.model("knn")
# show as HTML table
datatable(knn.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

K-Nearest Neighbors

KNN is a useful classification method in predicting qualitative variables. We built a KNN model using *train()* function with method *knn* from *caret* package and predicted the labels using *predict()* function. We also did 10-fold cross validation on the training set to find the best k value. In 10-fold cross validation, it randomly splits the training data into 10 non-overlapping individual sets with approximately equal size, each time a single fold is treated as testing set and the other nine folds are used to train the model, obtain the average of cross validation error and repeat this process for each tuning parameter k and finally choose the k with the smallest cross validation error. 

Moreover, we also tried to use *LOOCV* to reduce variance, but the computation intensity is large. Since our project focus on finding the model with smallest training sample and reduce the running time as much as possible. We decided to choose the 10-fold CV method to cross validate our models. Finally, we obtained misclassification rate with around 26% using sample 500 training set, around 25% using sample 1000 training set and around 22% using sample 2000 training set. 

Based on the scores that we got, we think the models using knn are performed well since we obtained low scores which are average around 0.18. Furthermore, we will choose the training dataset with sample size 1000 to train our final model since the average score of the three models with sample size 1000 is lower than the model trained by 500 and 2000 samples. The advantage of using KNN is that it is easy to interpret and explain. However, the computational cost is expensive as it needs to calculate each distance and stores all of the training data, in addition, the prediction accuracy is also less competitive in comparison to better supervised learning algorithm.


***

### Model 6 - Multinomial Logistic Regression 


```{r code_model6_development, eval = TRUE}
library(glmnet)
#Function for multinomial logistic regression model
multi.logistic <-function(TrainSet,TestSet){
   sample.size <- TrainSet[,.N]
   X <-as.matrix(TrainSet[,2:50])
   y <-as.matrix(TrainSet[,1])
   test <-as.matrix(TestSet[,2:50])
   start.time <-Sys.time() #record the start time
   multilog <-glmnet(X, y,family = "multinomial")
   #predict using multinomial logistic regression model
   predTest <-predict(multilog,newx=test,type="class") 
   end.time <-Sys.time() #record the end time
   running.time <-as.numeric(end.time-start.time) #model running time
   #Checking classification accuracy using the cross validated best column of prediction
   accuracy <-mean(predTest[,ncol(predTest)]== TestSet[,label])
   #Calculate Score
   score <-scoring.funnction(sample.size,running.time,accuracy)
   return(score)
   
}

```


```{r load_model6}
# get result table
multilogistic.result <- iteration.samples.each.model("multi.logistic")
# show as HTML table
datatable(multilogistic.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

Multinomial Logistic Regression

Since we want to predict a qualitative variable with 10 classes, multinomial logistic regression is a classifier that we knew could be useful for multiclassification. We built a multinomial logistic regression model using *glmnet()* function in *glmnet* package and predicted the labels using *predict()* function. The cross validation function is included in the *glmnet()*, therefore, we chose the parameter with the lowest training error. Finally, we obtained misclassification rate with around 30% using sample 500 training set, around 26% using sample 1000 training set and around 22% using sample 2000 training set. 

Based on the scores that we have gotten, we think the models using multinomial logistic regression do not have good prediction accuracy as compared to other method. We will choose the training dataset with sample size 1000 to train our final model since the average score of the three models with sample size 1000 is lower than the model trained by 500 and 2000 samples. The advantage of multinomial regression model is that it is easy to interpret the coefficients. The drawbacks of using multinomial regression model are poor prediction accuracy and requires logistic assumption.


***

### Model 7 - Classification Trees


```{r code_model7_development, eval = TRUE}
classification.tree<-function(TrainSet,TestSet){
  library(rpart)
  sample.size <- TrainSet[,.N]
  test <-as.matrix(TestSet[,2:50])
  start.time<-Sys.time()#record the start time
  #Fit classification tree model
  tree.model<-rpart(label~.,data=TrainSet,method = "class")
  tree.model.prune<-prune(tree.model,cp=0.01)
  #predict using testset
  predTest<-predict(tree.model.prune, newdata=TestSet, type="class")
  end.time<-Sys.time()#record the end time
  pred.table<-table(as.matrix(TestSet[,1]),predTest)
  #Calculate running time
  running.time<-as.numeric(end.time-start.time)
  #Calculate Accuracy of Model
  accuracy<-sum(diag(pred.table))/sum(pred.table)
  #Calculate score
  score <-scoring.funnction(sample.size,running.time,accuracy)
  return(score)
}

```

```{r load_model7}
# get result table
ClassificationTree.result <- iteration.samples.each.model("classification.tree")
# show as HTML table
datatable(ClassificationTree.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

Classification Trees

Classification trees is a commonly used model for classifying qualitative response. After applying the training dataset to build the tree model, our response will be classified into the most commonly occurring class. As for our dataset, the response variable is the cloth’s' label. We still apply the 9-training dataset to train the model separately and compare the points mentioned above. 

We build the model using *rpart* function in *rpart* package and predict the labels using function predict. To prevent overfitting, we pruned the classification tree and set the complexity parameter at 0.01 to guarantee the accuracy and prevent overfitting.

From the results we generated, the total scores do not have much differences with different training datasets, but the contribution of the numbers is different. For the largest datasets, the sample size scores and timing scores are large. Meanwhile, we can easily recognize that the large datasets have higher accuracy. The two points we mentioned above make the scores similar. Therefore, what size of dataset we will use depends on what our destination is. 


### Model 8 - Lasso Regression


```{r code_model8_development, eval = TRUE}
lasso.regression<-function(TrainSet,TestSet){
  library(glmnet)
  sample.size <- TrainSet[,.N]
  X <-as.matrix(TrainSet[,2:50])
  y <-as.matrix(TrainSet[,1])
  test <-as.matrix(TestSet[,2:50])
  start.time<-Sys.time()#record the start time
  #Fit the Lasso Regression Model
  model<-cv.glmnet(X,y, alpha=1, type.measure="class", family="multinomial")
  #Make prediction on Testing set
  model.predict<-predict(model, newx=test, type="class")
  end.time<-Sys.time()#record the end time
  #pred.table<-table(TestSet[,1],model.predict)
  #Calculate running time
  running.time<-as.numeric(end.time-start.time)
  #Calculate Accuracy of Model
  accuracy<-mean(model.predict[,ncol(model.predict)]== TestSet[,1])
  #Calculate score
  score <-scoring.funnction(sample.size,running.time,accuracy)
  return(score)
}
#Lasso_Regression(dat_500_1,fashion_test)
```

```{r load_model8}
# get result table
LassoRegression.result <- iteration.samples.each.model("lasso.regression")
# show as HTML table
datatable(LassoRegression.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

Lasso Regression

Lasso regression is an alternative linear model which applies least square method and adds a penalty to the formula. The form of the penalty is $\ell_{1}$ norm of a coefficient. Based on the theoretical knowledge, this method may not improve the accuracy of prediction, but reduce the variance. We apply *cv.glment* function in the package *glment* and set the parameter of alpha equal to 1. Also, we use the predict function to predict the labels. *cv.glmnet* can find the best $\lambda$ value based on 10-fold cross validation, then fit the lasso regression model. Cross validation may reduce the model variance.

The results show that the 2000 size training datasets have dramatically small score which means that the it is more practical to predict the label of clothes. It is abnormal that the timing scores is small for large dataset and the accuracy rises when size of dataset goes higher which both contribute to the high scores of larger datasets. Furthermore, we will definitely choose 2000 size dataset to make this prediction.


***

### Model 9 - Neural Networks

```{r code_model9_development, eval = TRUE}
neural.network<-function(TrainSet,TestSet){
  library(nnet)
  sample.size <- TrainSet[,.N]
  start.time<-Sys.time()
  #Fit the model
  model<-nnet(label~.,data=TrainSet, size=6)
  #predict on testing set
  nn.predict<-predict(model, TestSet, type= "class")
  #nn.table<-table(TestSet[,1], nn.predict)
  end.time<-Sys.time()
  #calculate the running time
  running.time<-as.numeric(end.time-start.time)
  #calculate the prediction accuracy
  accuracy <-mean(as.matrix(nn.predict)== TestSet[,1])
  #Calculate score
  score <-scoring.funnction(sample.size,running.time,accuracy)
  return(score)
}

```

```{r load_model9}
# get result table
NeuralNet.result <- iteration.samples.each.model("neural.network")
# show as HTML table
datatable(NeuralNet.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

Neural Networks

For the neural network model, we apply the function *nnet* from the package *nnet* and set the number of hidden layers as 6. Also, we make prediction of the labels of clothes by using predict function.  

From the result table, we can recognize that the data size score is going larger when size of the dataset grows. The time-consuming score also goes larger for large dataset. For the accuracy score, the largest datasets have least misclassification. Although the score shows that the 2000 datasets are more practical among the three sizes of dataset, the final score is still large comparing with other methods. Thus, this model may not competitive to this problem. In real pratice, Neural Networks should be a good approach to learning models, but our sample size is too small as well as just single layer applied here, leading to a bad performance compared to other models.


***

### Model 10 - Ensembled Model


```{r code_model10_development, eval = TRUE}
model.ensambling<-function(TrainSet, TestSet){
  library(randomForest)
  library(e1071)
  library(glmnet)
  #combine svm, random forest and ridge regression
  sample.size <- TrainSet[,.N] #sample size
  start.time <- Sys.time()# record start time
  #RandomForest Model
  rf.model <- randomForest(formula = label~. , data = TrainSet)
  rf.predTest <- as.matrix(predict(rf.model, TestSet, type = "response"))
  #SVM model
  mod.svm <-svm(label~.,data=TrainSet)
  svm.predTest <-as.matrix(predict(mod.svm,newdata=TestSet[,2:50]))
  #ridge regression
  rr.model <- cv.glmnet(x=data.matrix(TrainSet[,2:50]), y=TrainSet[,label], alpha=0,type.measure = "class",family="multinomial")
  # Predicting on test set
  rr.predTest <- as.matrix(predict(rr.model, data.matrix(TestSet[,2:50]), type="class", s="lambda.min"))
  #combine the three results
  predTest<-ifelse(rf.predTest==svm.predTest, rf.predTest, ifelse(rf.predTest==rr.predTest, rf.predTest,ifelse(svm.predTest==rr.predTest, svm.predTest, rf.predTest)))
  end.time<-Sys.time()
  running.time<-as.numeric(end.time-start.time)
  accuracy <-mean(as.matrix(predTest)== TestSet[,1])
  score <-scoring.funnction(sample.size,running.time,accuracy)
  return(score)
}
```

```{r load_model10}
# get result table
ensambling.result <- iteration.samples.each.model("model.ensambling")
# show as HTML table
datatable(ensambling.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

Ensembled Model

This model is a combination of *SVM model*, *random forest* and *ridge regression* model.  We train the data separately and make prediction on the testing set separately. The strategy to make the final prediction based on the prediction of the three methods mentioned above. It is a voting process. If there are two or three methods vote for the same type of label, then we make this label as our final decision. If all the three methods get distinct result, we depend on the prediction of random forest, because the misclassification level is lowest among these three methods.

From the result table, we can recognize that the size of the dataset is smaller, the score is smaller. Although the large dataset contributes an increase in the accuracy, the time score goes dramatically high.  For this ensambling method, we will fit the model and make prediction for three times, so the timing scores will dominate the final scores for the large datasets. Thus, the small size dataset will be more useful at this time based on our scoring method. If our destination is to get a lower misclassification method, the large datasets will be more practical.


***
 
## Models Results Summary

```{r model_results_summary}
# datatable summarize all the models reults
all.model.result <- rbind(random.forest.result,ridge.regression.result,boosted.regression.result,svm.result,knn.result,multilogistic.result,ClassificationTree.result,LassoRegression.result,NeuralNet.result,ensambling.result)
datatable(all.model.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

The table above summarize the total 90 models fitted above. For each of the 90 fitted models, the sample size proportion A, the running time score B and the misclassification rate C are computed and recorded. The overall points are computed based on the scoring function: points = 0.25$*$A+0.25$*$B+0.25$*$C.


## Scoreboard

```{r score_board_summary}
# score board that average the samples with same size and model
score.board<-all.model.result[,.(A=mean(A),B=mean(B),C=mean(C),Points=mean(Points)),by=c("Model","Sample Size")]
# sort points in increasing order
setorder(score.board, Points)
datatable(score.board[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

The table above report an overall scoreboard of the average results for the 30 combinations of Model and Sample Size. We evaluated the results of each model at a selected sample size by averaging the values of A, B, C, and Points across the 3 randomly sampled datasets. The values of A, B, C, and Points are all rounded to 4 decimal places, and the values on the scoreboard are sorted in increasing order of the Points scored.


## Performance evaluation

Top three models with the lowest points are **Model.svm** (1,2,7 total ranking =10); **Random. Forest** (3,4,6, total ranking=13); “Boosted.Regression” (10,11,12 total ranking =34). Which means, these three have the best performance over the three kind of sample size.

The four best models as well as their rankings will not change when the sample size switch from 500 to 1,000, which are “random. Forest”, “model. svm”, “model. ensambling”, “ridge. regression”. But when sample size comes to 2,000, the best model turns to svm and random. Forest comes to the second. Lasso regression and boosted regression also have a better performance. 

When we look at the timing score, classification tree, neural network and svm are always the best model under three different sample sizes. When sample size equals to 500 and 1,000, the lasso regression spend much more time (almost five time bigger) than the other models. As for the accuracy, ensambling, random forest and svm have the lowest misclassification rate with 500, 1,000, 2,000 sample size. Additionally, ensambling has a better performance under 500, and random forest comes to the best when sample size gets bigger. Classification tree and neural network always spend the longest time and have the lowest accuracy with the analysis when using this dataset with different sample size. 

The average misclassification rate decreases from 0.30898 to 0.25829 when the sample size grows from 500 to 2,000, Which indicates that we can get a higher accuracy with a bigger sample size. Average time spent without lasso regression increases from 0.055 to 0.138 when the sample size increases from 500 to 2,000. Therefore, we need more time to analyze a bigger data size. To sum up, when dealing with this dataset, “svm” and “random forest” will be strongly recommended to do the analysis because of their less time spending and higher accuracy. 


## Discussion

In conclusion, “SVM” and “random forest” are the most suitable methods that we found in predicting the labels in this case. These two methods perform really well in combination of three perspectives which are prediction accuracy, running time and training sample size. The scoring function put half of the weight on prediction accuracy, 25% on time and the remaining 25% on training sample size. If we give more weight on time and training sample size, SVM and random forest might no longer be the good choice, in that case, we might choose an algorithm with simple calculation, moderate prediction accuracy and performs especially well on small training data set. Therefore, the scoring formula that we used has great impact in our final decision.

In future, we want to explore more on other machine learning algorithms such as xgboost and Bayesian. Moreover, parameter selection is another important area that we want to improve as some of the models we used are very sensitive in choosing different parameters and we might want to come up with another way to evaluate our model instead of the scoring function that professor gave to us. 


## References

James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. *An Introduction to Statistical Learning: With Applications in R*. , 2013. Print.


