---
title: "Applied Data Science:  Midterm Project"
author: "Yawen Han, Anqi Wang, Xuran Jia"
date: "03/09/2019"
output:
  prettydoc::html_pretty:
  theme: cayman
highlight: github
---

```{r setup, include=FALSE}
set.seed(72)
knitr::opts_chunk$set(echo = TRUE, comment="", warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=55))
```

```{r libraries, echo = FALSE}
library(data.table)
library(DT)
```

```{r source_files}

```

```{r functions}

```



```{r load_data}
training.data.file <- "../Data/MNIST-fashion training set-49.csv"
testing.data.file <- "../Data/MNIST-fashion testing set-49.csv"
train.dat <- fread(input = training.data.file, verbose = FALSE)
test.dat <- fread(input = testing.data.file, verbose = FALSE)
```

```{r constants}
n.values <- c(500, 1000, 2000)
iterations <- 3
train.dat.size <- train.dat[,.N]
```

```{r clean_data}


```

```{r round_numerics_function}
#Function round numerical vars to required decimal
round.numerics <- function(x, digits){
  if(is.numeric(x)){
    x <- round(x = x, digits = digits)
  }
  return(x)
}
```


```{r generate_samples}
# Getting some random values to use here
find.sample.index <- function(train.dat, sample.size) {
  index <- 1:train.dat[,.N]
  sample1.index <- sample(x = index,size = sample.size,replace = FALSE)
  # Remove the sampled items from the vector of values
  sample2.index <- sample(x = index[-c(sample1.index)],size = sample.size,replace = FALSE)
  # Another sample, and another removal
  sample3.index <- sample(x = index[-c(sample1.index,sample2.index)],size = sample.size,replace = FALSE)
  return(list(sample1.index,sample2.index,sample3.index))
}

#change the datatype of label column as factor
train.dat[,label:=factor(label)]
test.dat[,label:=factor(label)]

#iteration for all possible sample sizes
for (i in 1:iterations){
  #call function  find.sample.index
  #get sample indexes for three samples with required sample size
  sample.index <- find.sample.index(train.dat, n.values[i])
  #get three samples
  for (j in 1:iterations){
    #name with pattern
    sample.name <- sprintf("dat_%d_%d", n.values[i], j)
    #assign samples with names
    assign(eval(sample.name), train.dat[unlist(sample.index[j]),])
  }
}
```



```{r iteration_sample_for_each_model}
# Function that iterate all sample sizes for the given model
# return a list and a datatable
iteration.samples.each.model <- function(model){
  #initialize a list to store model results
  ls = list()
  #iterarte all samples
  for (i in 1:iterations){
    for (j in 1:iterations){
      TrainSet <- sprintf("dat_%d_%d", n.values[i], j)
      # call model function to compute score
      score <- get(model)(get(TrainSet),test.dat)
      ls[3*(i-1)+j] <- list(c(model,n.values[i],TrainSet,score))
    }
  }
  #transform list of list to data.table
  dat.result= setDT(as.data.frame(t(as.data.frame(ls))))
  #set column names
  setnames(dat.result,c("Model","Sample Size","Data","A","B","C","Points"))
  #convert factor to numeric for columns "A","B","C","Points"
  dat.result[, c("A","B","C","Points"):=lapply(X=.SD, as.character), .SDcols=c("A","B","C","Points")]
  dat.result[, c("A","B","C","Points"):=lapply(X=.SD, as.numeric), .SDcols=c("A","B","C","Points")]
  return (dat.result)
}


```


```{r scoring_function}
##Function used to calculate the score of fitted model
scoring.funnction <- function(sample.size, running.time, accuracy){
  A <- sample.size/train.dat.size
  B <- min(running.time/60.0, 1)
  C <- 1-accuracy
  score <- 0.25*A + 0.25*B +0.5*C
  return (c(A,B,C,score))
}

```



## Introduction


### Model 1:  

```{r code_model1_development, eval = TRUE}
library(randomForest)
#Function for RandomForest Model
random.forest <- function(TrainSet,TestSet){
  sample.size <- TrainSet[,.N] #sample size
  #RandomForest Model
  start.time <- Sys.time()# record start time
  model <- randomForest(formula = label~. , data = TrainSet)
  # Predicting on Validation set
  predTest <- predict(model, TestSet, type = "response")
  end.time <- Sys.time()# record end time
  running.time <- as.numeric(end.time - start.time)  # model running time
  # Checking classification accuracy
  accuracy <- mean(predTest == TestSet[,label])
  #calculate score of fitted model
  score <- scoring.funnction(sample.size, running.time, accuracy)
  return (score)
}

```


```{r load_model1}
# get result table
random.forest.result <- iteration.samples.each.model("random.forest")
# show as HTML table
datatable(random.forest.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])

```

### Model 2:  


```{r code_model2_development, eval = TRUE}
library(glmnet)
#Function for RidgeRegression Model
ridge.regression <- function(TrainSet,TestSet){
  sample.size <- TrainSet[,.N]
  #RidgeRegression Model
  start.time <- Sys.time()# record start time
  model <- cv.glmnet(x=data.matrix(TrainSet[,2:50]), y=TrainSet[,label], alpha=0,type.measure = "class",family="multinomial")
  # Predicting on test set
  predTest <- predict(model, data.matrix(TestSet[,2:50]), type="class", s="lambda.min")
  end.time <- Sys.time()# record end tiime
  running.time <- as.numeric(end.time-start.time) # model running time
  # Checking classification accuracy
  accuracy <- mean(predTest == TestSet[,label]) 
  #calculate score of fitted model
  score <- scoring.funnction(sample.size, running.time, accuracy)
  return (score)
}
```


```{r load_model2}
# get result table
ridge.regression.result <- iteration.samples.each.model("ridge.regression")
# show as HTML table
datatable(ridge.regression.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

### Model 3:  


```{r code_model3_development, eval = TRUE}
library(gbm)
#Function for BoostedRegression Model
boosted.regression <- function(TrainSet,TestSet){
  sample.size <- TrainSet[,.N]
  #BoostedRegression Model
  start.time <- Sys.time()# record start time
  # fit BoostedRegression Model
  model <- gbm(label ~ ., data=TrainSet, distribution="multinomial", cv.folds=5)  
  # Predicting on test set
  predTest <- predict(model, newdata=TestSet, n.trees=100, type = "response")
  #print (predTest)
  end.time <- Sys.time()# record end tiime
  running.time <- as.numeric(end.time-start.time) # model running time
  predTest <- setDT(as.data.frame(predTest))
  
  predTest <- predTest[, colnames(.SD)[max.col(.SD, ties.method = "first")]]
  predTest <- sub(".100","",x=predTest)# remove pattern
  
  # Checking classification accuracy
  accuracy <- mean(predTest == TestSet[,label]) 
  #calculate score of fitted model
  score <- scoring.funnction(sample.size, running.time, accuracy)
  return (score)
}
```


```{r load_model3}
# get result table
boosted.regression.result <- iteration.samples.each.model("boosted.regression")
# show as HTML table
datatable(boosted.regression.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

### Model 4


```{r code_model4_development, eval = TRUE}

library(e1071)
#Function for svm model
model.svm <-function(TrainSet,TestSet){
   sample.size <- TrainSet[,.N]
   start.time <-Sys.time() #record start time
   #SVM model
   mod.svm <-svm(label~.,data=TrainSet)
   #Predicting on test set
   predTest <-predict(mod.svm,newdata=TestSet[,2:50])
   end.time <-Sys.time() #record end time
   running.time <-as.numeric(end.time-start.time) #model running time
   #Checking classification accuracy
   accuracy <- mean(predTest == TestSet[,label])
   #Calculate Score
   score <-scoring.funnction(sample.size,running.time,accuracy)
   return(score)
}
```


```{r load_model4}
# get result table
svm.result <- iteration.samples.each.model("model.svm")
# show as HTML table
datatable(svm.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

### Model 5


```{r code_model5_development, eval = TRUE}
library(caret)
#Function for knn model
model.knn <-function(TrainSet,TestSet){
    sample.size <- TrainSet[,.N]
    start.time <-Sys.time() #record the start time
    ctrl <- trainControl(method="repeatedcv",repeats = 10)  
    knnFit <- train(label ~ ., data = TrainSet, method = "knn", 
                trControl = ctrl, preProcess = c("center","scale")) #tune the parameter k with 10-fold cross validation
    predTest <-predict(knnFit, newdata = TestSet[,2:50]) #predict using the best k in knnFit
    end.time <-Sys.time() #record the end time
    running.time <-as.numeric(end.time-start.time)
    #Checking classification accuracy
    accuracy <- mean(predTest == TestSet[,label])
    #Calculate Score
    score <-scoring.funnction(sample.size,running.time,accuracy)
    return(score)
}

```

```{r load_model5}
# get result table
knn.result <- iteration.samples.each.model("model.knn")
# show as HTML table
datatable(knn.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

### Model 6


```{r code_model6_development, eval = TRUE}
library(glmnet)
#Function for multinomial logistic regression model
model.multilogistic <-function(TrainSet,TestSet){
   sample.size <- TrainSet[,.N]
   X <-as.matrix(TrainSet[,2:50])
   y <-as.matrix(TrainSet[,1])
   test <-as.matrix(TestSet[,2:50])
   start.time <-Sys.time() #record the start time
   multilog <-glmnet(X, y,family = "multinomial")
   #predict using multinomial logistic regression model
   predTest <-predict(multilog,newx=test,type="class") 
   end.time <-Sys.time() #record the end time
   running.time <-as.numeric(end.time-start.time) #model running time
   #Checking classification accuracy using the cross validated best column of prediction
   accuracy <-mean(predTest[,ncol(predTest)]== TestSet[,label])
   #Calculate Score
   score <-scoring.funnction(sample.size,running.time,accuracy)
   return(score)
   
}

```


```{r load_model6}
# get result table
multilogistic.result <- iteration.samples.each.model("model.multilogistic")
# show as HTML table
datatable(multilogistic.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```


### Model 7


```{r code_model7_development, eval = TRUE}

```

```{r load_model7}

```

### Model 8


```{r code_model8_development, eval = TRUE}

```

```{r load_model8}

```

### Model 9


```{r code_model9_development, eval = TRUE}

```

```{r load_model9}

```

### Model 10


```{r code_model10_development, eval = TRUE}

```

```{r load_model10}

```


```{r model_results_summary}
# datatable summarize all the models reults
all.model.result <- rbind(random.forest.result,ridge.regression.result,boosted.regression.result,svm.result,knn.result,multilogistic.result)
datatable(all.model.result[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```

```{r score_board_summary}
# score board that average the samples with same size and model
score.board<-all.model.result[,.(A=mean(A),B=mean(B),C=mean(C),Points=mean(Points)),by=c("Model","Sample Size")]
# sort points in increasing order
setorder(score.board, Points)
datatable(score.board[, lapply(X=.SD, FUN="round.numerics",digits=4)])
```


## Scoreboard

```{r scoreboard}

```

## Discussion


## References


